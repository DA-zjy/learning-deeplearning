{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f54d27-baef-44d6-8d35-feef28d9bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670b087f-8571-446f-937d-4cea7567bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "# 修改后的正确路径\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ccc48",
   "metadata": {},
   "source": [
    "数据预处理 (transforms):\n",
    "\n",
    "    transforms.ToTensor(): 神经网络的输入必须是 PyTorch 的 Tensor 格式。这张“配方”的作用就是将输入的 PIL 图像（一种 Python 图像库格式）或 numpy 数组转换成 Tensor。同时，它会自动将图像的像素值从 [0, 255] 的范围缩放到 [0.0, 1.0] 的范围，这有助于模型训练的稳定。\n",
    "\n",
    "    transforms.Normalize((0.5,), (0.5,)): 这是数据标准化。它会将数据的范围从 [0, 1] 变换到 [-1, 1]（计算方法是 (input - mean) / std，所以 (0-0.5)/0.5 = -1, (1-0.5)/0.5 = 1）。让数据分布在 0 附近，可以加快模型收敛速度。(0.5,) 括号里的逗号表示这是一个单元素的元组，因为 MNIST 是单通道的灰度图。\n",
    "\n",
    "    transforms.Compose([...]): 这像一个管道，它将多个预处理步骤串联起来，按顺序执行。\n",
    "\n",
    "数据集 (torchvision.datasets.MNIST):\n",
    "\n",
    "    root='./data': 指定数据集下载后存放的目录。\n",
    "\n",
    "    train=True: 表示我们加载的是训练集（60000 张图片）。train=False 表示加载测试集（10000 张图片）。\n",
    "\n",
    "    download=True: 如果在 root 目录下找不到数据，就自动下载。非常方便。\n",
    "\n",
    "    transform=transform: 将我们上面定义的预处理流程应用到每一张加载的图片上。\n",
    "\n",
    "数据加载器 (DataLoader):\n",
    "\n",
    "    为什么需要它？ 我们的训练集有 60000 张图片，不可能一次性全部加载到内存/显存中去训练，会直接爆掉。DataLoader 就是解决这个问题的。它像一个智能的“数据供给管道”。\n",
    "\n",
    "    dataset=...: 告诉 DataLoader 从哪个数据集中取数据。\n",
    "\n",
    "    batch_size=64: 这是批大小。DataLoader 每次不会只给我们一张图片，而是给我们一个包含 64 张图片和对应标签的“批次”。这是一种折中：既避免了单张图片训练的低效，也避免了全部数据训练的内存溢出。批处理训练也能让梯度下降更稳定。\n",
    "\n",
    "    shuffle=True: 这个对训练集至关重要！ shuffle=True 意味着在每个训练轮次（epoch）开始时，都会打乱整个数据集的顺序。这可以防止模型学习到数据的特定顺序，增强其泛化能力。而对于测试集，我们不需要打乱顺序，所以设为 False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694c3649-052a-41f2-8e0e-2af4b023b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleMLP(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleMLP()\n",
    "print(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e8af5",
   "metadata": {},
   "source": [
    "model.to(device): 这是 nn.Module 提供的强大方法。它会遍历模型中所有的参数（权重和偏置），并将它们全部转移到你指定的 device（GPU 或 CPU）的内存中。\n",
    "model = SimpleMLP() 就是调用类的构造函数，创建了一个我们刚刚设计的 SimpleMLP 的实例对象。\n",
    "x.view(...) 是用来改变 Tensor 形状的函数。\n",
    "\n",
    "输入 x 的原始形状是 [batch_size, 1, 28, 28] (批次大小, 通道数, 高, 宽)。\n",
    "\n",
    "nn.Linear 层要求输入是二维的 [batch_size, in_features]。所以我们需要把 [1, 28, 28] 这几维“拍扁”成一维的 784。\n",
    "\n",
    "-1 是一个非常方便的占位符。它告诉 PyTorch：“你帮我自动计算这个维度应该是多少”。在这里，它会自动被推断为 batch_size。所以，这行代码的作用就是把 x 的形状从 [batch_size, 1, 28, 28] 变成 [batch_size, 784]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c24871",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1a12b",
   "metadata": {},
   "source": [
    "损失函数：交叉熵损失\n",
    "优化器：Adam优化参数、学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b64bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch [1/5], Step [200/938], Loss: 0.6812\n",
      "Epoch [1/5], Step [400/938], Loss: 0.3588\n",
      "Epoch [1/5], Step [600/938], Loss: 0.3170\n",
      "Epoch [1/5], Step [800/938], Loss: 0.2871\n",
      "Epoch [2/5], Step [200/938], Loss: 0.2330\n",
      "Epoch [2/5], Step [400/938], Loss: 0.2207\n",
      "Epoch [2/5], Step [600/938], Loss: 0.1998\n",
      "Epoch [2/5], Step [800/938], Loss: 0.1877\n",
      "Epoch [3/5], Step [200/938], Loss: 0.1669\n",
      "Epoch [3/5], Step [400/938], Loss: 0.1522\n",
      "Epoch [3/5], Step [600/938], Loss: 0.1389\n",
      "Epoch [3/5], Step [800/938], Loss: 0.1423\n",
      "Epoch [4/5], Step [200/938], Loss: 0.1285\n",
      "Epoch [4/5], Step [400/938], Loss: 0.1255\n",
      "Epoch [4/5], Step [600/938], Loss: 0.1230\n",
      "Epoch [4/5], Step [800/938], Loss: 0.1155\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0966\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0972\n",
      "Epoch [5/5], Step [600/938], Loss: 0.1074\n",
      "Epoch [5/5], Step [800/938], Loss: 0.1034\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "num_epochs = 5\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 200 == 0:    # 每训练200个批次，打印一次平均损失\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 200:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8608bef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94.91%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total  = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8faad044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型权重已保存到: ../saved_models/mnist_mlp_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# 创建保存目录\n",
    "save_dir = '../saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(save_dir, 'mnist_mlp_model.pth')\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"✅ 模型权重已保存到: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
